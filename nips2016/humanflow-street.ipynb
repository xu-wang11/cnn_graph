{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "import sys, os        \n",
    "sys.path.insert(0, '..')\n",
    "from lib import models, graph, coarsening, utils\n",
    "import numpy as np\n",
    "import time\n",
    "from nips2016 import humantraffic\n",
    "from tensorflow.python import debug as tf_debug\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "np.random.seed(2017) \n",
    "# notification\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "\n",
    "# 配置显存大小\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#config = tf.ConfigProto(\n",
    "#         device_count = {'GPU': 0}\n",
    "#     )\n",
    "# sess = tf.Session(config=config)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Graphs.\n",
    "flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
    "flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n",
    "# TODO: change cgcnn for combinatorial Laplacians.\n",
    "flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
    "flags.DEFINE_integer('coarsening_levels', 4, 'Number of coarsened graphs.')\n",
    "\n",
    "# Directories.\n",
    "flags.DEFINE_string('dir_data', os.path.join('..', 'data', 'mnist'), 'Directory to store data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from CoreUtils.SendNotification import send_notification\n",
    "from CoreUtils import SendNotification\n",
    "SendNotification.http_config = '59.66.107.158'\n",
    "# SendNotification.http_config = '192.168.34.138'\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path is /home/csi/Git/HumanFlowPrediction/cnn_graph/nips2016\n"
     ]
    }
   ],
   "source": [
    "print('current path is {0}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ce112e237274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# A = graph.replace_random_edges(A, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# graphs, perm = coarsening.coarsen(A, levels=FLAGS.coarsening_levels, self_connections=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ce112e237274>\u001b[0m in \u001b[0;36mgrid_graph\u001b[0;34m(m, corners)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrid_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance_sklearn_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjacency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/Git/HumanFlowPrediction/cnn_graph/lib/graph.py\u001b[0m in \u001b[0;36mdistance_sklearn_metrics\u001b[0;34m(z, k, metric)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"\"\"Compute exact pairwise distances.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     d = sklearn.metrics.pairwise.pairwise_distances(\n\u001b[0;32m---> 36\u001b[0;31m             z, metric=metric, n_jobs=2)\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# k-NN graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     ret = Parallel(n_jobs=n_jobs, verbose=0)(\n\u001b[1;32m   1088\u001b[0m         \u001b[0mfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         for s in gen_even_slices(Y.shape[0], n_jobs))\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_initialize_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             return self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n\u001b[0;32m--> 540\u001b[0;31m                                            **self._backend_args)\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFallbackToBackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;31m# Recursively initialize the backend in case of requested fallback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mconfigure\u001b[0;34m(self, n_jobs, parallel, **backend_args)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# Make sure to free as much memory as possible before forking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemmapingPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbackend_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, temp_folder, max_nbytes, mmap_mode, forward_reducers, backward_reducers, verbose, context_id, prewarm, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             backward_reducers=backward_reducers)\n\u001b[1;32m    599\u001b[0m         \u001b[0mpoolargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpoolargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, forward_reducers, backward_reducers, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mpoolargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mpoolargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPicklingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpoolargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_queues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repopulate_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         self._worker_handler = threading.Thread(\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_repopulate_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PoolWorker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'added worker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csi/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "def sparse_matrix_element_wise_max(A, B):\n",
    "    BisBigger = A-B\n",
    "    BisBigger.data = np.where(BisBigger.data < 0, 1, 0)\n",
    "    return A - A.multiply(BisBigger) + B.multiply(BisBigger)\n",
    "def grid_graph(m, corners=False):\n",
    "    z = graph.grid(m)\n",
    "    dist, idx = graph.distance_sklearn_metrics(z, k=FLAGS.number_edges, metric=FLAGS.metric)\n",
    "    A = graph.adjacency(dist, idx)\n",
    "\n",
    "    # Connections are only vertical or horizontal on the grid.\n",
    "    # Corner vertices are connected to 2 neightbors only.\n",
    "    if corners:\n",
    "        import scipy.sparse\n",
    "        A = A.toarray()\n",
    "        A[A < A.max()/1.5] = 0\n",
    "        A = scipy.sparse.csr_matrix(A)\n",
    "        print('{} edges'.format(A.nnz))\n",
    "\n",
    "    print(\"{} > {} edges\".format(A.nnz//2, FLAGS.number_edges*m**2//2))\n",
    "    return A\n",
    "\n",
    "t_start = time.process_time()\n",
    "A = grid_graph(20, corners=False)\n",
    "# A = graph.replace_random_edges(A, 0)\n",
    "# graphs, perm = coarsening.coarsen(A, levels=FLAGS.coarsening_levels, self_connections=False)\n",
    "L = [graph.laplacian(A, normalized=True)] # [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "# graph.plot_spectrum(L)\n",
    "# del A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experiment init\n",
    "# DATA_SET_PATH='../../data/lndata'\n",
    "# DATA_SET_PATH='../../data/lndata_filter'\n",
    "DATA_SET_PATH='../../data/lndata_street'\n",
    "#DATA_SET_PATH='../../data/bjtaxi'\n",
    "# DATA_SET_PATH='../../data/shanxidata'\n",
    "seq_num = 4\n",
    "t_start = time.process_time()\n",
    "ht = humantraffic.HumanTraffic(DATA_SET_PATH)\n",
    "# train_data, val_data, test_data, train_labels, val_labels, test_labels, A1 = ht.load_bj_data(seq_num)\n",
    "train_data, val_data, test_data, train_labels, val_labels, test_labels, A1 = ht.load_lndata_street(seq_num)\n",
    "# train_data, val_data, test_data, train_labels, val_labels, test_labels, A1 = ht.load_split_ln_data(seq_num)\n",
    "A1 = A1.astype(np.float32)\n",
    "# A = sparse_matrix_element_wise_max(A, A1)\n",
    "# A = A.astype(np.float32)\n",
    "L = [graph.laplacian(A1, normalized=True)] # [graph.laplacian(A, normalized=True) for A in graphs]\n",
    "#del A\n",
    "train_data_ = np.zeros((train_data.shape[0], train_data.shape[1], train_data.shape[2]))\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "print('train_data shape: ', train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_labels.shape)\n",
    "plt.plot(train_labels[:, 69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "common = {}\n",
    "common['dir_name']       = 'mnist/'\n",
    "common['num_epochs']     = 100\n",
    "common['batch_size']     = 100\n",
    "common['decay_steps']    = train_data.shape[0] / common['batch_size']\n",
    "common['eval_frequency'] = 100\n",
    "common['brelu']          = 'b1relu'\n",
    "common['pool']           = 'mpool1'\n",
    "# C = max(mnist.train.labels) + 1  # number of classes\n",
    "model_perf = utils.model_perf()\n",
    "print(train_data.shape)\n",
    "# Common hyper-parameters for networks with one convolutional layer.\n",
    "common['regularization'] = 0\n",
    "common['dropout']        = 1\n",
    "common['learning_rate']  = 0.03\n",
    "common['decay_rate']     = 0.9\n",
    "common['momentum']       = 0.9\n",
    "common['F']              = [10]\n",
    "common['K']              = [20]\n",
    "common['p']              = [1]\n",
    "common['M']              = [train_data.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    params['filter'] = 'chebyshev5' # fourier\n",
    "    # params['filter'] = 'fourier'\n",
    "    \n",
    "    params['_nfilter'] = 32\n",
    "    params['_nres_layer_count'] = 3\n",
    "    params['K'] = [1]\n",
    "    params['C_0'] = train_data.shape[2]\n",
    "    params['model_name'] = 'ResGNN'\n",
    "    # params['model_name'] = 'GNN'\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.plot(train_target[:, 68, 1])\n",
    "plt.plot(train_pred[:, 68, 1])\n",
    "print(train_pred[:, 68, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "                     \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 4\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 6\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 8\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 10\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 12\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 14\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 16\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 18\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "if True:\n",
    "    \n",
    "    # sess = tf.Session(config=config)\n",
    "    start_time = time.time()\n",
    "    name = 'fgconv_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'fourier'\n",
    "    params['_nfilter'] = 24\n",
    "    params['_nres_layer_count'] = 20\n",
    "    params['K'] = [20]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    end_time = time.time()\n",
    "    print(\"total time {0} elapse...\\n\".format(end_time - start_time))\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    # send_notification('cnn_graph finished part 1', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.config.update({\"load_extensions\":{\"notify\":true}})\n",
    "Jupyter.notebook.config.update({\"load_extensions\":{\"theme_toggle\":true}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    name = 'spline_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'spline'\n",
    "    params['K'] = [10]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    send_notification('cnn_graph finished part 2', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    \n",
    "    name = 'chebyshev_softmax'\n",
    "    params = common.copy()\n",
    "    params['dir_name'] += name\n",
    "    #params['filter'] = 'chebyshev2' # fourier\n",
    "    params['filter'] = 'chebyshev5'\n",
    "    params['K'] = [5]\n",
    "    params['C_0'] = seq_num * 2\n",
    "    train_pred, test_pred =  model_perf.test(models.cgcnn(L, **params), name, params,\n",
    "                    train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "\n",
    "    train_pred, test_pred = ht.reverse_normalize(train_pred), ht.reverse_normalize(test_pred)\n",
    "    train_target, test_target = ht.reverse_normalize(train_labels), ht.reverse_normalize(test_labels)\n",
    "    #\n",
    "    # real_data = np.concatenate(train_labels, test_labels)\n",
    "    # pred_data = np.concatenate(train_pred, test_pred)\n",
    "    print(str(math.sqrt(np.sum((train_target - train_pred) ** 2) / (train_pred.shape[0] * train_pred.shape[1] * train_pred.shape[2]))))\n",
    "    print(str(math.sqrt(np.sum((test_target - test_pred) ** 2) / (test_pred.shape[0] * test_pred.shape[1] * test_pred.shape[2]))))\n",
    "    print('train finish...\\n')\n",
    "    send_notification('cnn_graph finished part 3', 'cnn_grpah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.plot(train_target[:, 68, 1])\n",
    "plt.plot(train_pred[:, 68, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.plot(test_target[:, 68, 1])\n",
    "plt.plot(test_pred[:, 68, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
